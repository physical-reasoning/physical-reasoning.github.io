---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

permalink: /
title: Home
layout: home
---

<figure>
	<div style="text-align:center">
		<img src="assets/img/banner.jpg" alt="A banner for the workshop" />
	</div>
</figure>
<br />

Much progress has been made on end-to-end learning for physical understanding and reasoning. If successful, understanding and reasoning about the visual and physical world promises far-reaching applications in robotics, machine vision, and the physical sciences. Despite this recent progress, our best artificial systems pale in comparison to the flexibility and generalization of human physical reasoning.

Neural information processing systems have shown promising empirical results on synthetic datasets, yet do not transfer well when deployed in novel scenarios (including the physical world). If physical understanding and reasoning techniques are to play a broader role in the physical world, they must be able to function across a wide variety of scenarios, including ones that might lie outside the training distribution. How can we design systems that satisfy these criteria?

Our workshop aims to investigate this broad question by bringing together experts from machine learning, the physical sciences, cognitive and developmental psychology, and robotics to investigate how these techniques may one day be employed in the real world. In particular, we aim to investigate the following questions:
1. What forms of inductive biases best enable the development of physical understanding techniques that are applicable to real-world problems?
2. How do we ensure that the outputs of a physical reasoning module are reasonable and physically plausible?
3. Is interpretability a necessity for physical understanding and reasoning techniques to be suitable to real-world problems?

Unlike end-to-end neural architectures that distribute bias across a large set of parameters, modern structured physical reasoning modules (differentiable physics engines, relational inductive biases, energy-conservation mechanisms, probabilistic programming tools) strive to maintain modularity and physical interpretability. In this workshop, we will discuss how these various architectures and inductive biases might help artificial systems learn generalizable and/or interpretable principles, and how these different facets can be combined to produce systems that solve real-world problems. 


